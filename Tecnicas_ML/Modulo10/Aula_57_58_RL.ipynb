{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOTRo4bxds7V"
   },
   "source": [
    "# Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDORHRU-d0Fl"
   },
   "source": [
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "- First thing, there is an agent that explores some space.\n",
    "- As it goes, the agent learns by receiving feedback for each action.\n",
    "- An action that leads to the correct end result receives a reward, and a bad action is either ignored or punished.\n",
    "- Examples: Pac-Man, Cat & Mouse game\n",
    "\n",
    "### Q-Learning\n",
    "An implementation of reinforcement learning.\n",
    "The objects are:\n",
    "- A set of environmental states \\( s \\)\n",
    "- A set of possible actions in those states \\( a \\)\n",
    "- A value of each state/action \\( Q \\)\n",
    "\n",
    "Start off with \\( Q \\) values of 0, then explore the space. If a bad thing happens after a given state/action, reduce its \\( Q \\), but if a good thing happens, increase its \\( Q \\).\n",
    "\n",
    "### The exploration problem\n",
    "How do we efficiently explore all of the possible states?\n",
    "#### The simple approach:\n",
    "Always choose the action for a given state with the highest \\( Q \\). If there is a tie, choose at random.\n",
    "#### Better way: introduce an $( \\epsilon )$ term\n",
    "- If a random number is less than $( \\epsilon )$, don’t follow the highest \\( Q \\), but choose at random.\n",
    "- That way, exploration never totally stops.\n",
    "- Choosing an $( \\epsilon )$ can be tricky.\n",
    "#### Markov decision processes\n",
    "- A mathematical framework for modeling decision-making in situations where outcomes are partly random.\n",
    "- Basically the same thing above but more formal.\n",
    "- States still are s and s', transition between states are $P_a$(s, s') and Q are a reward function $R_a$(s,s')\n",
    "\n",
    "#### Dinamic Programming\n",
    "- From my backgorund as a competitive programmer, this is basicaly an method that computes large problems (like Fibonacci numbers) using smaller ones, that are alredy computed and stored in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zla7vVIFdn-b"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4QWZLPY63lK",
    "outputId": "36a973f0-a034-4c43-8f52-0061cc08394c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurivzz/.local/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/yurivzz/.local/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "# Seta a seed para termos resultados aleatorios \"controlados\"\n",
    "random.seed(1234)\n",
    "\n",
    "# Utilizando gym 0.25.2 não tive o problema relatado no card \n",
    "# Mas essa versão ta com os dias contados\n",
    "# Como mudava algumas coisas do codigo do curso, preferi não alterar\n",
    "streets = gym.make('Taxi-v3').env\n",
    "streets.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Enqsa_FI786I",
    "outputId": "1ab0e180-f8d4-4bfd-a419-1c40b53a888c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurivzz/.local/lib/python3.11/site-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Codifica um estado inicial específico (linha de táxi, coluna de táxi, posição do passageiro, destino)\n",
    "initial_state = streets.encode(2, 3, 2, 0)\n",
    "\n",
    "# Define o estado inicial do ambiente para o estado codificado\n",
    "streets.s = initial_state\n",
    "test = streets.render(mode='ansi')\n",
    "print(f'{test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1fJcerqbu5o",
    "outputId": "2143ca36-0915-46aa-e52d-d77513cc56bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabela de recompensas de cada decisão no estado inicial\n",
    "streets.P[initial_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKXypYOb1uG",
    "outputId": "f09f9193-3c4c-4f5c-e841-c3b49175b17c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tabela de recompensa para cada estado\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Parametros de aprendizado, utilizados na formula do algoritmo Q-learning\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.6\n",
    "exploration = 0.1\n",
    "\n",
    "# Epocas de cada aprendizado\n",
    "epochs = 10000\n",
    "\n",
    "# Loop q passa por cada epoca\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "\n",
    "    # Loop de aprendizado, para apenas quando concluir o desafio (pegar e levar o passageiro)\n",
    "    while not done:\n",
    "      # condicional aleatorio que dita se vai pegar o maior Q ou uma açao aleatoria\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample() # Explore a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "\n",
    "        # Aplica a ação escolhida\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "        # essa é a parte onde o algoritmo realmente aprende, adaptando a q_table com uma equação\n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        q_table[state, action] = new_q\n",
    "\n",
    "        state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ay4B7wMPx6q1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10 Step 12\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "# Loop de 11 viagens\n",
    "for tripnum in range(1, 11):\n",
    "  # Seta uma nova viagem\n",
    "    state = streets.reset()\n",
    "\n",
    "  #  variaveis de controle\n",
    "    done = False\n",
    "    trip_length = 0\n",
    "\n",
    "    # Loop de uma viagem\n",
    "    while not done and trip_length < 25:\n",
    "      # Escolhe o maior q da tabela já treinada, para essse estado\n",
    "        action = np.argmax(q_table[state])\n",
    "        # Aplica a ação\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        # Limpa o display\n",
    "        clear_output(wait=True)\n",
    "        # Printa a viagem\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "        print(streets.render(mode='ansi'))\n",
    "        # Sleep para conseguirmos ver o que acontece\n",
    "        sleep(.5)\n",
    "        # Avança para o proximo estado\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "\n",
    "    sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meda2oxU_bz7"
   },
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuRzxpk8A6xo",
    "outputId": "4f2c4e9c-c47d-4056-e02d-2940d641db83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def training(q_table, learning_rate, discount_factor, exploration, epochs):\n",
    "  # Loop q passa por cada epoca\n",
    "  start = time.time()\n",
    "  for taxi_run in range(epochs):\n",
    "      state = streets.reset()\n",
    "      done = False\n",
    "\n",
    "      # Loop de aprendizado, para apenas quando concluir o desafio (pegar e levar o passageiro)\n",
    "      while not done:\n",
    "        # condicional aleatorio que dita se vai pegar o maior Q ou uma açao aleatoria\n",
    "          random_value = random.uniform(0, 1)\n",
    "          if (random_value < exploration):\n",
    "              action = streets.action_space.sample() # Explore a random action\n",
    "          else:\n",
    "              action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "\n",
    "          # Aplica a ação escolhida\n",
    "          next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "          # essa é a parte onde o algoritmo realmente aprende, adaptando a q_table com uma equação\n",
    "          prev_q = q_table[state, action]\n",
    "          next_max_q = np.max(q_table[next_state])\n",
    "          new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "          q_table[state, action] = new_q\n",
    "\n",
    "          state = next_state\n",
    "  end = time.time()\n",
    "  print(f'Tempo de treino: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "wHpaD8lmByaO"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Função para testar o aprendizado (meio q depende mto de como o reset acontece mas eh isso ai), medindo o tempo médio de N testes\n",
    "def test(q_table, n):\n",
    "  t = []\n",
    "  # Loop de 11 viagens\n",
    "  for tripnum in range(1, n):\n",
    "    # Seta uma nova viagem\n",
    "      state = streets.reset()\n",
    "\n",
    "    #  variaveis de controle\n",
    "      done = False\n",
    "      trip_length = 0\n",
    "\n",
    "      # Loop de uma viagem\n",
    "      start = time.time()\n",
    "      while not done:\n",
    "        # Acho que 100 ainda ta generoso\n",
    "          if (trip_length >= 100):\n",
    "            print('Modelo falho')\n",
    "            return 0\n",
    "          # Escolhe o maior q da tabela já treinada, para essse estado\n",
    "          action = np.argmax(q_table[state])\n",
    "          # Aplica a ação\n",
    "          next_state, reward, done, info = streets.step(action)\n",
    "          state = next_state\n",
    "          trip_length += 1\n",
    "\n",
    "      end = time.time()\n",
    "      t.append(end-start)\n",
    "  print(f'tempo médio: {np.mean(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zahdnFxpGhti",
    "outputId": "98d8002e-f106-4ce4-fe30-287ae954377a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempo médio: 0.0005383780508330374\n"
     ]
    }
   ],
   "source": [
    "# Controle\n",
    "test(q_table, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTFeUpv3EgMK",
    "outputId": "53302fda-ae7e-462e-d933-128b288e90ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 1.2981035709381104\n",
      "Modelo falho\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste com apenas 10 epochs\n",
    "q_table1 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.1\n",
    "df = 0.6\n",
    "ex = 0.1\n",
    "ep = 50\n",
    "\n",
    "training(q_table1, lr, df, ex, ep)\n",
    "test(q_table1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wc_koeFdG6f0",
    "outputId": "c356ab0a-3f77-492f-c4e7-e8194796cfe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 9.899224758148193\n",
      "tempo médio: 0.00027807071955517087\n"
     ]
    }
   ],
   "source": [
    "# Aumentando o learning rate\n",
    "q_t2 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.3\n",
    "df = 0.6\n",
    "ex = 0.1\n",
    "ep = 10000\n",
    "\n",
    "training(q_t2, lr, df, ex, ep)\n",
    "test(q_t2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgYPP7NGHx6U",
    "outputId": "ec272ea7-1126-4ca3-ef9c-ca6b76ca7ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 0.3913130760192871\n",
      "tempo médio: 0.0004013596159039122\n"
     ]
    }
   ],
   "source": [
    "# Aumentando o learning rate e diminuindo epochs\n",
    "q_t3 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.3\n",
    "df = 0.6\n",
    "ex = 0.1\n",
    "ep = 20\n",
    "\n",
    "training(q_t3, lr, df, ex, ep)\n",
    "test(q_t2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AtItoD7IAkk",
    "outputId": "5b67e036-bc69-4a6b-e328-eb07d17e5dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 16.718200206756592\n",
      "Modelo falho\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diminuindo Discout Factor\n",
    "q_t4 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.1\n",
    "df = 0.2\n",
    "ex = 0.1\n",
    "ep = 10000\n",
    "\n",
    "training(q_t4, lr, df, ex, ep)\n",
    "test(q_t4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRyGnAVzIzGi",
    "outputId": "5a6395a8-3591-476a-f4cc-05011909dfa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 8.982004165649414\n",
      "tempo médio: 0.000612923593232126\n"
     ]
    }
   ],
   "source": [
    "# Diminuindo Discout Factor e aumentando learning rate\n",
    "q_t5 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.3\n",
    "df = 0.4\n",
    "ex = 0.1\n",
    "ep = 10000\n",
    "\n",
    "training(q_t5, lr, df, ex, ep)\n",
    "test(q_t5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8ZMa5PaI7DF",
    "outputId": "8d31cb46-98af-49bf-c79f-ea3d2ecb5409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 11.211408138275146\n",
      "tempo médio: 0.00046297516485657356\n"
     ]
    }
   ],
   "source": [
    "# Aumentando Discout Factor\n",
    "q_t6 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.1\n",
    "df = 0.8\n",
    "ex = 0.1\n",
    "ep = 10000\n",
    "\n",
    "training(q_t6, lr, df, ex, ep)\n",
    "test(q_t6, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-MZ9ATtJ29m",
    "outputId": "f111b366-bf2a-45ad-89f5-10cb202ca94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treino: 10.036751747131348\n",
      "Modelo falho\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aumentando Discout Factor e learning rate\n",
    "q_t7 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "lr = 0.3\n",
    "df = 0.8\n",
    "ex = 0.1\n",
    "ep = 10000\n",
    "\n",
    "training(q_t7, lr, df, ex, ep)\n",
    "test(q_t7, 100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
