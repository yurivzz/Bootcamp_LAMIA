{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory\n"
      ],
      "metadata": {
        "id": "eOTRo4bxds7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Reinforcement Learning\n",
        "\n",
        "- First thing, there is an agent that explores some space.\n",
        "- As it goes, the agent learns by receiving feedback for each action.\n",
        "- An action that leads to the correct end result receives a reward, and a bad action is either ignored or punished.\n",
        "- Examples: Pac-Man, Cat & Mouse game\n",
        "\n",
        "### Q-Learning\n",
        "An implementation of reinforcement learning.\n",
        "The objects are:\n",
        "- A set of environmental states \\( s \\)\n",
        "- A set of possible actions in those states \\( a \\)\n",
        "- A value of each state/action \\( Q \\)\n",
        "\n",
        "Start off with \\( Q \\) values of 0, then explore the space. If a bad thing happens after a given state/action, reduce its \\( Q \\), but if a good thing happens, increase its \\( Q \\).\n",
        "\n",
        "### The exploration problem\n",
        "How do we efficiently explore all of the possible states?\n",
        "#### The simple approach:\n",
        "Always choose the action for a given state with the highest \\( Q \\). If there is a tie, choose at random.\n",
        "#### Better way: introduce an $( \\epsilon )$ term\n",
        "- If a random number is less than $( \\epsilon )$, don’t follow the highest \\( Q \\), but choose at random.\n",
        "- That way, exploration never totally stops.\n",
        "- Choosing an $( \\epsilon )$ can be tricky.\n",
        "#### Markov decision processes\n",
        "- A mathematical framework for modeling decision-making in situations where outcomes are partly random.\n",
        "- Basically the same thing above but more formal.\n",
        "- States still are s and s', transition between states are $P_a$(s, s') and Q are a reward function $R_a$(s,s')\n",
        "\n",
        "#### Dinamic Programming\n",
        "- From my backgorund as a competitive programmer, this is basicaly an method that computes large problems (like Fibonacci numbers) using smaller ones, that are alredy computed and stored in memory.\n"
      ],
      "metadata": {
        "id": "CDORHRU-d0Fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "Zla7vVIFdn-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "d4QWZLPY63lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a973f0-a034-4c43-8f52-0061cc08394c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "227"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "import gym\n",
        "import random\n",
        "\n",
        "# Seta a seed para termos resultados aleatorios \"controlados\"\n",
        "random.seed(1234)\n",
        "\n",
        "# Curiosamente não tive problemas como os indicados no card\n",
        "streets = gym.make('Taxi-v3').env\n",
        "streets.reset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Codifica um estado inicial específico (linha de táxi, coluna de táxi, posição do passageiro, destino)\n",
        "initial_state = streets.encode(2, 3, 2, 0)\n",
        "\n",
        "# Define o estado inicial do ambiente para o estado codificado\n",
        "streets.s = initial_state\n",
        "test = streets.render(mode='ansi')\n",
        "print(f'{test}')"
      ],
      "metadata": {
        "id": "Enqsa_FI786I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab0e180-f8d4-4bfd-a419-1c40b53a888c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabela de recompensas de cada decisão no estado inicial\n",
        "streets.P[initial_state]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1fJcerqbu5o",
        "outputId": "2143ca36-0915-46aa-e52d-d77513cc56bd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 368, -1, False)],\n",
              " 1: [(1.0, 168, -1, False)],\n",
              " 2: [(1.0, 288, -1, False)],\n",
              " 3: [(1.0, 248, -1, False)],\n",
              " 4: [(1.0, 268, -10, False)],\n",
              " 5: [(1.0, 268, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tabela de recompensa para cada estado\n",
        "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "# Parametros de aprendizado, utilizados na formula do algoritmo Q-learning\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.6\n",
        "exploration = 0.1\n",
        "\n",
        "# Epocas de cada aprendizado\n",
        "epochs = 10000\n",
        "\n",
        "# Loop q passa por cada epoca\n",
        "for taxi_run in range(epochs):\n",
        "    state = streets.reset()\n",
        "    done = False\n",
        "\n",
        "    # Loop de aprendizado, para apenas quando concluir o desafio (pegar e levar o passageiro)\n",
        "    while not done:\n",
        "      # condicional aleatorio que dita se vai pegar o maior Q ou uma açao aleatoria\n",
        "        random_value = random.uniform(0, 1)\n",
        "        if (random_value < exploration):\n",
        "            action = streets.action_space.sample() # Explore a random action\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
        "\n",
        "        # Aplica a ação escolhida\n",
        "        next_state, reward, done, info = streets.step(action)\n",
        "\n",
        "        # essa é a parte onde o algoritmo realmente aprende, adaptando a q_table com uma equação\n",
        "        prev_q = q_table[state, action]\n",
        "        next_max_q = np.max(q_table[next_state])\n",
        "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
        "        q_table[state, action] = new_q\n",
        "\n",
        "        state = next_state\n",
        "\n"
      ],
      "metadata": {
        "id": "yCKXypYOb1uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09f9193-3c4c-4f5c-e841-c3b49175b17c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação das bibliotecas\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "# Loop de 11 viagens\n",
        "for tripnum in range(1, 11):\n",
        "  # Seta uma nova viagem\n",
        "    state = streets.reset()\n",
        "\n",
        "  #  variaveis de controle\n",
        "    done = False\n",
        "    trip_length = 0\n",
        "\n",
        "    # Loop de uma viagem\n",
        "    while not done and trip_length < 25:\n",
        "      # Escolhe o maior q da tabela já treinada, para essse estado\n",
        "        action = np.argmax(q_table[state])\n",
        "        # Aplica a ação\n",
        "        next_state, reward, done, info = streets.step(action)\n",
        "        # Limpa o display\n",
        "        clear_output(wait=True)\n",
        "        # Printa a viagem\n",
        "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
        "        print(streets.render(mode='ansi'))\n",
        "        # Sleep para conseguirmos ver o que acontece\n",
        "        sleep(.5)\n",
        "        # Avança para o proximo estado\n",
        "        state = next_state\n",
        "        trip_length += 1\n",
        "\n",
        "    sleep(2)\n"
      ],
      "metadata": {
        "id": "ay4B7wMPx6q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activity"
      ],
      "metadata": {
        "id": "meda2oxU_bz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def training(q_table, learning_rate, discount_factor, exploration, epochs):\n",
        "  # Loop q passa por cada epoca\n",
        "  start = time.time()\n",
        "  for taxi_run in range(epochs):\n",
        "      state = streets.reset()\n",
        "      done = False\n",
        "\n",
        "      # Loop de aprendizado, para apenas quando concluir o desafio (pegar e levar o passageiro)\n",
        "      while not done:\n",
        "        # condicional aleatorio que dita se vai pegar o maior Q ou uma açao aleatoria\n",
        "          random_value = random.uniform(0, 1)\n",
        "          if (random_value < exploration):\n",
        "              action = streets.action_space.sample() # Explore a random action\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
        "\n",
        "          # Aplica a ação escolhida\n",
        "          next_state, reward, done, info = streets.step(action)\n",
        "\n",
        "          # essa é a parte onde o algoritmo realmente aprende, adaptando a q_table com uma equação\n",
        "          prev_q = q_table[state, action]\n",
        "          next_max_q = np.max(q_table[next_state])\n",
        "          new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
        "          q_table[state, action] = new_q\n",
        "\n",
        "          state = next_state\n",
        "  end = time.time()\n",
        "  print(f'Tempo de treino: {end-start}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRzxpk8A6xo",
        "outputId": "4f2c4e9c-c47d-4056-e02d-2940d641db83"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Função para testar o aprendizado (meio q depende mto de como o reset acontece mas eh isso ai), medindo o tempo médio de N testes\n",
        "def test(q_table, n):\n",
        "  t = []\n",
        "  # Loop de 11 viagens\n",
        "  for tripnum in range(1, n):\n",
        "    # Seta uma nova viagem\n",
        "      state = streets.reset()\n",
        "\n",
        "    #  variaveis de controle\n",
        "      done = False\n",
        "      trip_length = 0\n",
        "\n",
        "      # Loop de uma viagem\n",
        "      start = time.time()\n",
        "      while not done:\n",
        "        # Acho que 100 ainda ta generoso\n",
        "          if (trip_length >= 100):\n",
        "            print('Modelo falho')\n",
        "            return 0\n",
        "          # Escolhe o maior q da tabela já treinada, para essse estado\n",
        "          action = np.argmax(q_table[state])\n",
        "          # Aplica a ação\n",
        "          next_state, reward, done, info = streets.step(action)\n",
        "          state = next_state\n",
        "          trip_length += 1\n",
        "\n",
        "      end = time.time()\n",
        "      t.append(end-start)\n",
        "  print(f'tempo médio: {np.mean(t)}')"
      ],
      "metadata": {
        "id": "wHpaD8lmByaO"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Controle\n",
        "test(q_table, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zahdnFxpGhti",
        "outputId": "98d8002e-f106-4ce4-fe30-287ae954377a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tempo médio: 0.0005383780508330374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste com apenas 10 epochs\n",
        "q_table1 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.1\n",
        "df = 0.6\n",
        "ex = 0.1\n",
        "ep = 50\n",
        "\n",
        "training(q_table1, lr, df, ex, ep)\n",
        "test(q_table1, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTFeUpv3EgMK",
        "outputId": "53302fda-ae7e-462e-d933-128b288e90ab"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 1.2981035709381104\n",
            "Modelo falho\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aumentando o learning rate\n",
        "q_t2 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.3\n",
        "df = 0.6\n",
        "ex = 0.1\n",
        "ep = 10000\n",
        "\n",
        "training(q_t2, lr, df, ex, ep)\n",
        "test(q_t2, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc_koeFdG6f0",
        "outputId": "c356ab0a-3f77-492f-c4e7-e8194796cfe7"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 9.899224758148193\n",
            "tempo médio: 0.00027807071955517087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aumentando o learning rate e diminuindo epochs\n",
        "q_t3 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.3\n",
        "df = 0.6\n",
        "ex = 0.1\n",
        "ep = 20\n",
        "\n",
        "training(q_t3, lr, df, ex, ep)\n",
        "test(q_t2, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgYPP7NGHx6U",
        "outputId": "ec272ea7-1126-4ca3-ef9c-ca6b76ca7ae2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 0.3913130760192871\n",
            "tempo médio: 0.0004013596159039122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diminuindo Discout Factor\n",
        "q_t4 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.1\n",
        "df = 0.2\n",
        "ex = 0.1\n",
        "ep = 10000\n",
        "\n",
        "training(q_t4, lr, df, ex, ep)\n",
        "test(q_t4, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AtItoD7IAkk",
        "outputId": "5b67e036-bc69-4a6b-e328-eb07d17e5dab"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 16.718200206756592\n",
            "Modelo falho\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diminuindo Discout Factor e aumentando learning rate\n",
        "q_t5 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.3\n",
        "df = 0.4\n",
        "ex = 0.1\n",
        "ep = 10000\n",
        "\n",
        "training(q_t5, lr, df, ex, ep)\n",
        "test(q_t5, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyGnAVzIzGi",
        "outputId": "5a6395a8-3591-476a-f4cc-05011909dfa8"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 8.982004165649414\n",
            "tempo médio: 0.000612923593232126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aumentando Discout Factor\n",
        "q_t6 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.1\n",
        "df = 0.8\n",
        "ex = 0.1\n",
        "ep = 10000\n",
        "\n",
        "training(q_t6, lr, df, ex, ep)\n",
        "test(q_t6, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8ZMa5PaI7DF",
        "outputId": "8d31cb46-98af-49bf-c79f-ea3d2ecb5409"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 11.211408138275146\n",
            "tempo médio: 0.00046297516485657356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aumentando Discout Factor e learning rate\n",
        "q_t7 = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
        "\n",
        "lr = 0.3\n",
        "df = 0.8\n",
        "ex = 0.1\n",
        "ep = 10000\n",
        "\n",
        "training(q_t7, lr, df, ex, ep)\n",
        "test(q_t7, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-MZ9ATtJ29m",
        "outputId": "f111b366-bf2a-45ad-89f5-10cb202ca94f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treino: 10.036751747131348\n",
            "Modelo falho\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    }
  ]
}